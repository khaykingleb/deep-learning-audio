---
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html
# For fine-grained control over checkpointing behavior

# class LitModel(L.LightningModule):
#     def training_step(self, batch, batch_idx):
#         self.log("val_loss", x)

# saves top-K checkpoints based on "val_loss" metric
# checkpoint_callback = ModelCheckpoint(
#     save_top_k=10,
#     monitor="val_loss",
#     mode="min",
#     dirpath="my/path/",
#     filename="sample-mnist-{epoch:02d}-{val_loss:.2f}",
# )

# saves last-K checkpoints based on "global_step" metric
# checkpoint_callback = ModelCheckpoint(
#     save_top_k=10,
#     monitor="global_step",
#     mode="max",
#     dirpath="my/path/",
#     filename="sample-mnist-{epoch:02d}-{global_step}",
# )

# trainer = Trainer(callbacks=[checkpoint_callback])
# checkpoint_callback.best_model_path

# It is recommended that you pass formatting options to filename to include the monitored metric
# like shown in the example above. Otherwise, if save_top_k >= 2 and enable_version_counter=True (default),
# a version is appended to the filename to prevent filename collisions. You should not rely on the appended
#  version to retrieve the top-k model, since there is no relationship between version count and model
# performance. For example, filename-v2.ckpt doesnâ€™t necessarily correspond to the top-2 model.

# By default, the ModelCheckpoint callback saves model weights, optimizer states, etc.,
# but in case you have limited disk space or just need the model weights to be saved you can specify save_weights_only=True.

# By default, the ModelCheckpoint will save files into the Trainer.log_dir.
# It gives you the ability to specify the dirpath and filename for your checkpoints.
# Filename can also be dynamic so you can inject the metrics that are being logged using log().

model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
