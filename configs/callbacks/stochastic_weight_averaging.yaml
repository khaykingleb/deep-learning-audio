# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.StochasticWeightAveraging.html

# Stochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost.
# This can be used with both non-trained and trained models.
# The SWA procedure smooths the loss landscape thus making it harder to end up in a local minimum during optimization.
# For a more detailed explanation of SWA and how it works, read this post by the PyTorch team.
# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/

# trainer = Trainer(callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)])

stochastic_weight_averaging:
  _target_: lightning.pytorch.callbacks.StochasticWeightAveraging
  swa_lrs: 1e-2
