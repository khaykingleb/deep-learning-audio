---
# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html
# Used to monitor a metric and stop the training when no improvement is observed

# class LitModel(LightningModule):
#     def validation_step(self, batch, batch_idx):
#         loss = ...
#         self.log("val_loss", loss)
#
# early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=False, mode="min")
# trainer = Trainer(callbacks=[early_stop_callback])

early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  # Quantity to monitor (must be specified)
  monitor:
  # Minimum change in the monitored quantity to qualify as an improvement
  min_delta: 0.0
  # Number of checks with no improvement after which training will be stopped
  patience: 3
  # Verbosity mode
  verbose: false
  # Whether to crash the training if `monitor` is not found in the validation metrics.
  strict: true
  # If True, will check if the monitored quantity is a finite number
  check_finite: true
  # Stop training immediately once the monitored quantity reaches this threshold
  stopping_threshold:
  # Stop training as soon as the monitored quantity becomes worse than this threshold
  divergence_threshold:
  # Whether to run early stopping at the end of the training epoch
  # If this is False, then the check runs at the end of the validation
  check_on_train_epoch_end:
  # When set True, logs the status of the early stopping callback only for rank 0 process
  log_rank_zero_only: false
