---
# https://lightning.ai/docs/pytorch/stable/logging/wandb.html

# from lightning.pytorch.loggers import WandbLogger
# wandb_logger = WandbLogger(project="MNIST", log_model="all")
# trainer = Trainer(logger=wandb_logger)
# log gradients and model topology
# wandb_logger.watch(model)

wandb:
  _target_: lightning.pytorch.loggers.WandbLogger
  # Display name for the run
  # If not set, the name will be generated by wandb
  name:
  # Path where data is saved
  save_dir: ${root_dir}/wandb
  # Sets the version, mainly used to resume a previous run
  version:
  # Run offline (data can be streamed later to wandb servers)
  offline: false
  # Enables or explicitly disables anonymous logging
  anonymous:
  # The name of the project to which this run will belong
  # If not set, the environment variable `WANDB_PROJECT` will be used as a fallback
  # If both are not set, it defaults to `lightning_logs`
  project:
  # Log checkpoints created by :class:`~lightning.pytorch.callbacks.ModelCheckpoint`
  # as W&B artifacts. `latest` and `best` aliases are automatically set.
  # * if `log_model == 'all'`, checkpoints are logged during training.
  # * if `log_model == True`, checkpoints are logged at the end of training, except when
  #   `lightning.pytorch.callbacks.ModelCheckpoint.save_top_k == -1`
  #   which also logs every checkpoint during training.
  # * if `log_model == False` (default), no checkpoint is logged
  log_model: false
  # A string to put at the beginning of metric keys
  prefix:
  # Name of the model checkpoint artifact being logged
  checkpoint_name:
