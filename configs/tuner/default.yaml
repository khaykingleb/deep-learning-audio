---
# Batch size auto-scaling: find the largest batch size that fits into memory
# See https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#batch-size-finder
scale_batch_size:
  # NB: not yet supported for DDP or any of its variations
  # NB: often yields a better estimation of the gradients, but may also result in longer training time
  use: false

  # Find batch size by growing it exponentially ("power") or with binary search ("binsearch")
  mode: power

# Learning rate finder
# See https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#learning-rate-finder
scale_lr:
  # NB: only works with models having a single optimizer
  # NB: with DDP, since all the processes run in isolation, only process with global_rank=0 will make
  # the decision to stop the learning rate finder and broadcast its results to all other ranks
  use: false

  scale_lr_mode: exponential
