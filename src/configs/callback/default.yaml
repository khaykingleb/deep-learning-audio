

# The EarlyStopping callback can be used to monitor a metric and stop the training when no improvement is observed.
# class LitModel(LightningModule):
#     def validation_step(self, batch, batch_idx):
#         loss = ...
#         self.log("val_loss", loss)

# trainer = Trainer(callbacks=[EarlyStopping(monitor="val_loss", mode="min")])
# early_stop_callback = EarlyStopping(monitor="val_accuracy", min_delta=0.00, patience=3, verbose=False, mode="max")


# Whenever the .fit() function gets called, the Trainer will print the weights summary for the LightningModule.
# To add the child modules to the summary add a ModelSummary:
# from lightning.pytorch.callbacks import ModelSummary
# trainer = Trainer(callbacks=[ModelSummary(max_depth=-1)])


#  helpful technique to detect bottlenecks is to ensure that you’re using the full capacity of your accelerator (GPU/TPU/HPU). This can be measured with the DeviceStatsMonitor:
# from lightning.pytorch.callbacks import DeviceStatsMonitor
# trainer = Trainer(callbacks=[DeviceStatsMonitor()])
# CPU metrics will be tracked by default on the CPU accelerator. To enable it for other accelerators set DeviceStatsMonitor(cpu_stats=True). To disable logging CPU metrics, you can specify DeviceStatsMonitor(cpu_stats=False).

# For fine-grained control over checkpointing behavior, use the ModelCheckpoint object
# from lightning.pytorch.callbacks import ModelCheckpoint
# checkpoint_callback = ModelCheckpoint(dirpath="my/path/", save_top_k=2, monitor="val_loss")
# trainer = Trainer(callbacks=[checkpoint_callback])
# trainer.fit(model)
# checkpoint_callback.best_model_path
# Any value that has been logged via self.log in the LightningModule can be monitored.
# class LitModel(L.LightningModule):
#     def training_step(self, batch, batch_idx):
#         self.log("my_metric", x)
# # 'my_metric' is now able to be monitored
# checkpoint_callback = ModelCheckpoint(monitor="my_metric")
# # saves top-K checkpoints based on "val_loss" metric
# checkpoint_callback = ModelCheckpoint(
#     save_top_k=10,
#     monitor="val_loss",
#     mode="min",
#     dirpath="my/path/",
#     filename="sample-mnist-{epoch:02d}-{val_loss:.2f}",
# )
# # saves last-K checkpoints based on "global_step" metric
# # make sure you log it inside your LightningModule
# checkpoint_callback = ModelCheckpoint(
#     save_top_k=10,
#     monitor="global_step",
#     mode="max",
#     dirpath="my/path/",
#     filename="sample-mnist-{epoch:02d}-{global_step}",
# )
# It is recommended that you pass formatting options to filename to include the monitored metric
# like shown in the example above. Otherwise, if save_top_k >= 2 and enable_version_counter=True (default),
# a version is appended to the filename to prevent filename collisions. You should not rely on the appended
#  version to retrieve the top-k model, since there is no relationship between version count and model
# performance. For example, filename-v2.ckpt doesn’t necessarily correspond to the top-2 model.

# By default, the ModelCheckpoint callback saves model weights, optimizer states, etc.,
#  but in case you have limited disk space or just need the model weights to be saved you can specify save_weights_only=True.

# By default, the ModelCheckpoint will save files into the Trainer.log_dir.
# It gives you the ability to specify the dirpath and filename for your checkpoints.
# Filename can also be dynamic so you can inject the metrics that are being logged using log().


# Stochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost.
# This can be used with both non-trained and trained models.
# The SWA procedure smooths the loss landscape thus making it harder to end up in a local minimum during optimization.
# For a more detailed explanation of SWA and how it works, read this post by the PyTorch team.
# Enable Stochastic Weight Averaging using the callback
# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
trainer = Trainer(callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)])
